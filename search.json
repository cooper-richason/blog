[
  {
    "objectID": "posts/4- Deconstructing Decision Trees/index.html",
    "href": "posts/4- Deconstructing Decision Trees/index.html",
    "title": "Deconstructing Decision Trees",
    "section": "",
    "text": "I first learned about decision trees when I took Jeremy Howard’s incredible deep learning course, Practical Deep Learning for Coders in 2023, Jeremy made such a convincing argument in favor of using decision trees, especially as model when starting project, because of the benefits they offer:\n\nThey are interoperable - It is very easy to understand why a decision is made and what are the most important factors in making a decision. This can be really valuable if you need to be able to explain how decision are made to stakeholders, but even if you don’t end up using a tree model as your final product, starting with can be invaluable when one is starting a project and needs to get a feel for the data and experiment with with features.\nThey are robust – Unlike other models like logistic regression, tree based models are do not require data normalization, can handle missing values, and they can handle both categorical and numerical features with ease.\nThey are quick to build – Because of there robust nature and how simple they are under the hood, one doesn’t need to do a bunch of leg work before one can get a model running.\n\nHopefully those benefits have enticed you enough to want to learn more about decision trees! To begin let’s start with a single tree model, aka a decision tree.\n\nHow Do Decision Trees Work?\nTree based models work by making a series of binary splits of the data. A binary spit is basically just a yes/no or true/false question about the data like “is a value in a column greater than X”, but by doing this over and over we can slowly split the data into purer groups if we choose good splits. To find the best split at each step, the model looks at all of the values available in each column and finds the combination does the best job of separating into the chunk of data.\nFor example, let’s assume we are working with a dataset about high school students and what type of college they are going to next year. To make the first split, the model looks at all of the information and finds that the whether the student goes to a private high school (“HS”) is the best predictor of whether a student goes to a private university:\n\n\n\n\n\nflowchart TD\n        A[\"Starting Dataset\"]\n        A -- Private HS --&gt; C([\"Predict Private\"])\n        A -- Public  HS--&gt; D([\"Public HS Students\"])\n\n\n\n\n\n\nLet’s assume that the model tries split the public high school students group since that group has the lower accuracy and that the model learns that whether the student had an SAT score greater or less than 1800 is the best predictor of whether a public school student will go to a private or public university:\n\n\n\n\n\nflowchart TD\n        A[\"Starting Dataset\"]\n        A -- Private HS --&gt; C([\"Predict Private\"])\n        A -- Public  HS--&gt; D[\"Public HS Students\"]\n       D -- less than 1800 --&gt; E([\"Predict Public\"])\n      D -- 1800 and higher--&gt; F([\"Predict Private\"])\n\n\n\n\n\n\nNow we have a decision tree based on off two splits! When running a model in the real world, we will probably want to have have more than two splits since the more splits you have, the better the model is able to make, up to a point. Eventually, we need to stop splitting the tree so that we don’t overfit the model to our data. With python packages like sci-kit learn, the model will continue to split the tree over and over until it is not able to make any more splits because of stopping criteria you specify. Some examples of stopping criteria are the total depth of the tree or the number of nodes is a tree, but the ones I typically prefer are the minimum size of a resulting leaf or specifying the minimum number of records needed to make a new split.\nLastly, I should probably mention the terminology used. When talking about decision trees, we call each of the split points a “node”, with the starting node being called the root node and the subsequent nodes the decision nodes. We then refer to the final data chucks as the leaves of the tree, and these are with ovals in the diagram.\n\n\nExample with Code\nTerms Covered:\n\nBinary Splits:\n\nTrue or False questions that split the data into two groups. Each split is determined by what split will do the best job of purifying the data.\n\nDecision Tree:\n\nA model that makes predictions by making a series of binary splits\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/5- blog-post-ideas-for-data-scientists/index.html",
    "href": "posts/5- blog-post-ideas-for-data-scientists/index.html",
    "title": "Blog Post Ideas for Data Scientists",
    "section": "",
    "text": "If you’re a Data Scientists like me, you have been probably been given the advice of starting a blog. The only downside is that creating a blog is a lot of work! You have to build a website, come up with ideas for posts, write up something that is hopefully interesting and helpful, and share your work where it will be seen.\nI can’t really help you with most of that, but I would love to share my homework on the kinds of content that you could share/write.\nI’m going to mainly focus on content around sharing one’s educational journey, but this ideas can be adapted to people of all skill sets."
  },
  {
    "objectID": "posts/5- blog-post-ideas-for-data-scientists/index.html#general-ideas",
    "href": "posts/5- blog-post-ideas-for-data-scientists/index.html#general-ideas",
    "title": "Blog Post Ideas for Data Scientists",
    "section": "General Ideas:",
    "text": "General Ideas:\n\n1. Model Overview:\nWrite up an article explaining how a model works. This could maybe be a quick overview or a deep dive depending on how much time you are willing to put into it.\nValue to You: Forces you to deeply understand the algorithm, and demonstrates to others that you can breakdown complicated topics.\nValue to the Reader: For readers who are unfamiliar with the model, a well written article could be help them grasp the fundamentals of the model and give them an intuition for how it works. For others, this could be a good article to refresh their knowledge.\nExamples: - TSMixer: The Latest Forecasting Model by Google\n\n\n2. Model Walkthrough:\nAfter writing up an article explain how a model theoretically works, some readers may want to dive a little deeper and learn how to actually implement that model. That’s where sharing an accompanying article walking through actually using the model will come in handy. I also find that having skeleton projects set up the way I like to work can be really handy, so this could also be seen as a templating effort to help yourself down the line.\nValue to You: Re-enforces your knowledge of the model and gives you the opportunity to sent up a project template for the future.\nValue to the Reader: Shows them how to implement the model and gives them a easy way to tinker.\n\n\n3. Model Benchmarking:\nAnother coding related blog post could be running similar models on the same dataset or several datasets.\nValue to You: If there is a relatively new model that you are interested in, this could be a great way to contextualize the new model amongst models you are already familiar with.\nValue to the Reader: This can be a great way to help readers get a feel for what models they should start with for a given problem."
  },
  {
    "objectID": "posts/5- blog-post-ideas-for-data-scientists/index.html#additional-ideas",
    "href": "posts/5- blog-post-ideas-for-data-scientists/index.html#additional-ideas",
    "title": "Blog Post Ideas for Data Scientists",
    "section": "Additional Ideas:",
    "text": "Additional Ideas:\n\nFirst Thoughts with a new tool\nNew Things I’ve Learned about a tool\nThe Hows: How I did __ or How I used __ to do __ or How I built _ with __\nMy First __ Project"
  },
  {
    "objectID": "notes/agents/huggingface-course/p1.html",
    "href": "notes/agents/huggingface-course/p1.html",
    "title": "Unit 1 - Introduction",
    "section": "",
    "text": "Notes taken from HuggingFace.co Agent Course.\n\nWhat is an Agent?\n\nAgents are systems that use AI models, typically a LLM, that uses the tools provided to it to execute tasks defined by the user.\nThe brains of the operation is the AI model which uses its natural language abilities to take in the user’s instructions, reasons about what the user might want and plans how it can use the tools it has access to to meet the objective.\nAgents vary in how much “agency” they have:\n\nSimple processors where their output doesn’t control the programs flow. (i.e. summarizing the outputs from a automated process)\nRouter Agents where it just has basic controls over the program flow path\nTool Calling Agents that can run a single tool\nMulti step agents that can sequence tools and chain their outputs together\nMulti-Agent systems where multiple agents work together to complete a tasks\n\nSince Agents are powered by LLMs, they can only really generate text. All of the other stuff they do is through the tools they use like generating images or making a graph through writing python code.\nAgents can do any tasks that a tool/function can be written to do:\n\nMake a graph\nRun SQL\nSend a slack message\nWrite data to a Google Sheet\n\nThis means that the design of the tools is really important and directly impacts what an agent can do and how well they do it\n\n\n\nAgents and LLMs\n\nLLMs are AI models that excel at understanding and generating human language\nRecent LLMs have been built on the Transformer architecture, a method pioneer by Google with BERT in 2018\n\n\nThere are 3 types of transformers:\n\nEncoders: An encoder-based Transformer takes text (or other data) as input and outputs a dense representation (or embedding) of that text.\n\nExample: BERT from Google\nUse Cases: Text classification, semantic search, Named Entity Recognition\nTypical Size: Millions of parameters\n\nDecoders: A decoder-based Transformer focuses on generating new tokens to complete a sequence, one token at a time.\n\nExample: Llama from Meta\nUse Cases: Text generation, chatbots, code generation\nTypical Size: Billions (in the US sense, i.e., 10^9) of parameters\n\nSeq2Seq (Encoder–Decoder) - A sequence-to-sequence Transformer combines an encoder and a decoder. The encoder first processes the input sequence into a context representation, then the decoder generates an output sequence.\n\nExample: T5, BART\nUse Cases: Translation, Summarization, Paraphrasing\nTypical Size: Millions of parameters\n\nFrom HuggingFace.co\n\n\n\nThe most common approach for LLMS is to use decoders, so they generate text one token at a time\nThat means in order to get more than just a character or word fragment out of a model, the models need to pass the “autoregress”, aka pass the output back in as part of the input. Models keep doing this until they hit a special token to stop.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Cooper, and I’m currently a student at UW-Madison pursuing a degree in Economics while specializing in Data Science. My journey has taken an exciting turn towards harnessing the power of data to solve complex problems.\nIn addition to my academic pursuits, I’m actively engaged in a data science role at RehabPath where I use my skills to help people find mental health and addiction treatment. I’m deeply passionate about leveraging data to create positive change and am constantly seeking new opportunities to apply my skills and expand my knowledge. Let’s connect and explore how we can collaborate to tackle challenges with the power of data!\n\n\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Discoveries",
    "section": "",
    "text": "I am a believer in sharing what you’ve learned, sometimes the hard way, with others so they can accelerate their learning and avoid the mistakes I made. Here, I aim to share everything I learn that might interest, benefit, or amuse others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post Ideas for Data Scientists\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2025\n\n\nCooper Richason\n\n\n\n\n\n\n\n\n\n\n\n\nDeconstructing Decision Trees\n\n\n\ntree models\n\ndecision trees\n\n\n\nA beginner’s guide to using decision trees and how to use them\n\n\n\n\n\nJul 13, 2025\n\n\nCooper Richason\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Machine Learning Project: Fun or RUN\n\n\n\nPython\n\nFast.ai\n\nMachine Learning\n\nGradio\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nCooper Richason\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Cooper, and I’m currently a student at UW-Madison pursuing a degree in Economics while specializing in Data Science. My journey has taken an exciting turn towards harnessing the power of data to solve complex problems.\nIn addition to my academic pursuits, I’m actively engaged in a data science role at RehabPath where I use my skills to help people find mental health and addiction treatment. I’m deeply passionate about leveraging data to create positive change and am constantly seeking new opportunities to apply my skills and expand my knowledge. Let’s connect and explore how we can collaborate to tackle challenges with the power of data.\n\n\n Back to top"
  },
  {
    "objectID": "notes/llms/huggingface-course/p1.html",
    "href": "notes/llms/huggingface-course/p1.html",
    "title": "Unit 1 - Introduction",
    "section": "",
    "text": "Notes taken from HuggingFace.co Agent Course.\n\nWhat is an Agent?\n\nAgents are systems that use AI models, typically a LLM, that uses the tools provided to it to execute tasks defined by the user.\nThe brains of the operation is the AI model which uses its natural language abilities to take in the user’s instructions, reasons about what the user might want and plans how it can use the tools it has access to to meet the objective.\nAgents vary in how much “agency” they have:\n\nSimple processors where their output doesn’t control the programs flow. (i.e. summarizing the outputs from a automated process)\nRouter Agents where it just has basic controls over the program flow path\nTool Calling Agents that can run a single tool\nMulti step agents that can sequence tools and chain their outputs together\nMulti-Agent systems where multiple agents work together to complete a tasks\n\nSince Agents are powered by LLMs, they can only really generate text. All of the other stuff they do is through the tools they use like generating images or making a graph through writing python code.\nAgents can do any tasks that a tool/function can be written to do:\n\nMake a graph\nRun SQL\nSend a slack message\nWrite data to a Google Sheet\n\nThis means that the design of the tools is really important and directly impacts what an agent can do and how well they do it\n\n\n\nAgents and LLMs\n\nLLMs are AI models that excel at understanding and generating human language\nRecent LLMs have been built on the Transformer architecture, a method pioneer by Google with BERT in 2018\n\n\nThere are 3 types of transformers:\n\nEncoders: An encoder-based Transformer takes text (or other data) as input and outputs a dense representation (or embedding) of that text.\n\nExample: BERT from Google\nUse Cases: Text classification, semantic search, Named Entity Recognition\nTypical Size: Millions of parameters\n\nDecoders: A decoder-based Transformer focuses on generating new tokens to complete a sequence, one token at a time.\n\nExample: Llama from Meta\nUse Cases: Text generation, chatbots, code generation\nTypical Size: Billions (in the US sense, i.e., 10^9) of parameters\n\nSeq2Seq (Encoder–Decoder) - A sequence-to-sequence Transformer combines an encoder and a decoder. The encoder first processes the input sequence into a context representation, then the decoder generates an output sequence.\n\nExample: T5, BART\nUse Cases: Translation, Summarization, Paraphrasing\nTypical Size: Millions of parameters\n\nFrom HuggingFace.co\n\n\n\nThe most common approach for LLMS is to use decoders, so they generate text one token at a time\nThat means in order to get more than just a character or word fragment out of a model, the models need to pass the “autoregress”, aka pass the output back in as part of the input. Models keep doing this until they hit a special token to stop.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/1- Run or Fun/index.html",
    "href": "posts/1- Run or Fun/index.html",
    "title": "My First Machine Learning Project: Fun or RUN",
    "section": "",
    "text": "Welcome to my latest adventure in the world of data science! I’ve recently started watching Jeremy Howard’s amazing lecture series on deep learning, Practical Deep Learning for Coders, and I’ve been feeling very inspired by Jeremy to play around with deep learning and simply try things.\nFor my first deep learning project, I decided to create an image classifier, since Jermey had several examples on these, and I eventually landed on the idea of a movie poster classifier! My goals with this project are to:"
  },
  {
    "objectID": "posts/1- Run or Fun/index.html#try-the-ai-yourself",
    "href": "posts/1- Run or Fun/index.html#try-the-ai-yourself",
    "title": "My First Machine Learning Project: Fun or RUN",
    "section": "Try the AI Yourself!",
    "text": "Try the AI Yourself!\n\n\n\n\nGitHub Repository\n\nHuggingface Space\n\n\nFuture Horizons: Expanding “Fun or RUN”\nMy journey doesn’t stop here! I’m planning to enhance the model’s accuracy, broaden its genre recognition capabilities, and upload it to Huggingface Spaces. My next step involves using web scraping to gather a more diverse and accurate collection of movie posters from IMDb, taking advantage of their extensive genre listings.\n\n\nWrapping Up\nThis project has been an incredible learning experience, blending my love for movies with my passion for data science. It’s fascinating to see how AI can interpret visual media, and I’m excited to see where this path leads. I encourage you to try out Fun or RUN and share your thoughts. Have you worked on a similar project? Let’s exchange ideas and learn from each other!"
  }
]