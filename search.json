[
  {
    "objectID": "posts/1- Run or Fun/index.html",
    "href": "posts/1- Run or Fun/index.html",
    "title": "My First Machine Learning Project: Fun or RUN",
    "section": "",
    "text": "Welcome to my latest adventure in the world of data science! I’ve recently started watching Jeremy Howard’s amazing lecture series on deep learning, Practical Deep Learning for Coders, and I’ve been feeling very inspired by Jeremy to play around with deep learning and simply try things.\nFor my first deep learning project, I decided to create an image classifier, since Jermey had several examples on these, and I eventually landed on the idea of a movie poster classifier! My goals with this project are to:"
  },
  {
    "objectID": "posts/1- Run or Fun/index.html#try-the-ai-yourself",
    "href": "posts/1- Run or Fun/index.html#try-the-ai-yourself",
    "title": "My First Machine Learning Project: Fun or RUN",
    "section": "Try the AI Yourself!",
    "text": "Try the AI Yourself!\n\n\n\n\nGitHub Repository\n\nHuggingface Space\n\n\nFuture Horizons: Expanding “Fun or RUN”\nMy journey doesn’t stop here! I’m planning to enhance the model’s accuracy, broaden its genre recognition capabilities, and upload it to Huggingface Spaces. My next step involves using web scraping to gather a more diverse and accurate collection of movie posters from IMDb, taking advantage of their extensive genre listings.\n\n\nWrapping Up\nThis project has been an incredible learning experience, blending my love for movies with my passion for data science. It’s fascinating to see how AI can interpret visual media, and I’m excited to see where this path leads. I encourage you to try out Fun or RUN and share your thoughts. Have you worked on a similar project? Let’s exchange ideas and learn from each other!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Cooper, and I’m currently a student at UW-Madison pursuing a degree in Economics while specializing in Data Science. My journey has taken an exciting turn towards harnessing the power of data to solve complex problems.\nIn addition to my academic pursuits, I’m actively engaged in a data science role at RehabPath where I use my skills to help people find mental health and addiction treatment. I’m deeply passionate about leveraging data to create positive change and am constantly seeking new opportunities to apply my skills and expand my knowledge. Let’s connect and explore how we can collaborate to tackle challenges with the power of data.\n\n\n Back to top"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "In an effort to share what I learn with others and give back to the community, I post notes that I take here if I think they will be of value to others.\n\n\n\n\n\n\n\n\n\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Discoveries",
    "section": "",
    "text": "I am a believer in sharing what you’ve learned, sometimes the hard way, with others so they can accelerate their learning and avoid the mistakes I made. Here, I aim to share everything I learn that might interest, benefit, or amuse others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeconstructing Decision Trees\n\n\n\ntree models\n\ndecision trees\n\n\n\nA beginner’s guide to using decision trees and how to use them\n\n\n\n\n\nJul 13, 2025\n\n\nCooper Richason\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Machine Learning Project: Fun or RUN\n\n\n\nPython\n\nFast.ai\n\nMachine Learning\n\nGradio\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nCooper Richason\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Cooper, and I’m currently a student at UW-Madison pursuing a degree in Economics while specializing in Data Science. My journey has taken an exciting turn towards harnessing the power of data to solve complex problems.\nIn addition to my academic pursuits, I’m actively engaged in a data science role at RehabPath where I use my skills to help people find mental health and addiction treatment. I’m deeply passionate about leveraging data to create positive change and am constantly seeking new opportunities to apply my skills and expand my knowledge. Let’s connect and explore how we can collaborate to tackle challenges with the power of data!\n\n\n Back to top"
  },
  {
    "objectID": "notes/template/index.html",
    "href": "notes/template/index.html",
    "title": "Cooper Richason",
    "section": "",
    "text": "Just a test thing\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/4- Deconstructing Decision Trees/index.html",
    "href": "posts/4- Deconstructing Decision Trees/index.html",
    "title": "Deconstructing Decision Trees",
    "section": "",
    "text": "I first learned about decision trees when I took Jeremy Howard’s incredible deep learning course, Practical Deep Learning for Coders in 2023, Jeremy made such a convincing argument in favor of using decision trees, especially as model when starting project, because of the benefits they offer:\n\nThey are interoperable - It is very easy to understand why a decision is made and what are the most important factors in making a decision. This can be really valuable if you need to be able to explain how decision are made to stakeholders, but even if you don’t end up using a tree model as your final product, starting with can be invaluable when one is starting a project and needs to get a feel for the data and experiment with with features.\nThey are robust – Unlike other models like logistic regression, tree based models are do not require data normalization, can handle missing values, and they can handle both categorical and numerical features with ease.\nThey are quick to build – Because of there robust nature and how simple they are under the hood, one doesn’t need to do a bunch of leg work before one can get a model running.\n\nHopefully those benefits have enticed you enough to want to learn more about decision trees! To begin let’s start with a single tree model, aka a decision tree.\n\nHow Do Decision Trees Work?\nTree based models work by making a series of binary splits of the data. A binary spit is basically just a yes/no or true/false question about the data like “is a value in a column greater than X”, but by doing this over and over we can slowly split the data into purer groups if we choose good splits. To find the best split at each step, the model looks at all of the values available in each column and finds the combination does the best job of separating into the chunk of data.\nFor example, let’s assume we are working with a dataset about high school students and what type of college they are going to next year. To make the first split, the model looks at all of the information and finds that the whether the student goes to a private high school (“HS”) is the best predictor of whether a student goes to a private university:\n\n\n\n\n\nflowchart TD\n        A[\"Starting Dataset\"]\n        A -- Private HS --&gt; C([\"Predict Private\"])\n        A -- Public  HS--&gt; D([\"Public HS Students\"])\n\n\n\n\n\n\nLet’s assume that the model tries split the public high school students group since that group has the lower accuracy and that the model learns that whether the student had an SAT score greater or less than 1800 is the best predictor of whether a public school student will go to a private or public university:\n\n\n\n\n\nflowchart TD\n        A[\"Starting Dataset\"]\n        A -- Private HS --&gt; C([\"Predict Private\"])\n        A -- Public  HS--&gt; D[\"Public HS Students\"]\n       D -- less than 1800 --&gt; E([\"Predict Public\"])\n      D -- 1800 and higher--&gt; F([\"Predict Private\"])\n\n\n\n\n\n\nNow we have a decision tree based on off two splits! When running a model in the real world, we will probably want to have have more than two splits since the more splits you have, the better the model is able to make, up to a point. Eventually, we need to stop splitting the tree so that we don’t overfit the model to our data. With python packages like sci-kit learn, the model will continue to split the tree over and over until it is not able to make any more splits because of stopping criteria you specify. Some examples of stopping criteria are the total depth of the tree or the number of nodes is a tree, but the ones I typically prefer are the minimum size of a resulting leaf or specifying the minimum number of records needed to make a new split.\nLastly, I should probably mention the terminology used. When talking about decision trees, we call each of the split points a “node”, with the starting node being called the root node and the subsequent nodes the decision nodes. We then refer to the final data chucks as the leaves of the tree, and these are with ovals in the diagram.\n\n\nExample with Code\nTerms Covered:\n\nBinary Splits:\n\nTrue or False questions that split the data into two groups. Each split is determined by what split will do the best job of purifying the data.\n\nDecision Tree:\n\nA model that makes predictions by making a series of binary splits\n\n\n\n\n\n\n Back to top"
  }
]