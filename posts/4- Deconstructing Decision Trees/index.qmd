---
title: Deconstructing Decision Trees
description: A beginner's guide to using decision trees and how to use them
author: Cooper Richason
date: '2025-07-13'
date-modified: '2025-07-13'
categories:
- tree models
- decision trees
draft: false
toc: true
---

![](default_header.png)

------------------------------------------------------------------------

I first learned about decision trees when I took Jeremy Howard's incredible deep learning course, [Practical Deep Learning for Coders](https://course.fast.ai/) in 2023, Jeremy made such a convincing argument in favor of using decision trees, especially as model when starting project, because of the benefits they offer:

1.  **They are interoperable** - It is very easy to understand why a decision is made and what are the most important factors in making a decision. This can be really valuable if you need to be able to explain how decision are made to stakeholders, but even if you don't end up using a tree model as your final product, starting with can be invaluable when one is starting a project and needs to get a feel for the data and experiment with with features.

2.  **They are robust** -- Unlike other models like logistic regression, tree based models are do not require data normalization, can handle missing values, and they can handle both categorical and numerical features with ease.

3.  **They are quick to build** -- Because of there robust nature and how simple they are under the hood, one doesn't need to do a bunch of leg work before one can get a model running.

Hopefully those benefits have enticed you enough to want to learn more about decision trees! To begin let's start with a single tree model, aka a decision tree.

### How Do Decision Trees Work?

Tree based models work by making a series of **binary splits** of the data. A binary spit is basically just a yes/no or true/false question about the data like "is a value in a column greater than X", but by doing this over and over we can slowly split the data into purer groups if we choose good splits. To find the best split at each step, the model looks at all of the values available in each column and finds the combination does the best job of separating into the chunk of data.

For example, let's assume we are working with a dataset about high school students and what type of college they are going to next year. To make the first split, the model looks at all of the information and finds that the **whether the student goes to a private high school ("HS")** is the best predictor of whether a student goes to a private university:

```{mermaid}
flowchart TD
        A["Starting Dataset"]
        A -- Private HS --> C(["Predict Private"])
        A -- Public  HS--> D(["Public HS Students"])
```

Let's assume that the model tries split the public high school students group since that group has the lower accuracy and that the model learns that whether the student had an SAT score greater or less than 1800 is the best predictor of whether a public school student will go to a private or public university:

```{mermaid}
flowchart TD
        A["Starting Dataset"]
        A -- Private HS --> C(["Predict Private"])
        A -- Public  HS--> D["Public HS Students"]
       D -- less than 1800 --> E(["Predict Public"])
      D -- 1800 and higher--> F(["Predict Private"])
```

Now we have a decision tree based on off two splits! When running a model in the real world, we will probably want to have have more than two splits since the more splits you have, the better the model is able to make, up to a point. Eventually, we need to stop splitting the tree so that we don't overfit the model to our data. With python packages like sci-kit learn, the model will continue to split the tree over and over until it is not able to make any more splits because of stopping criteria you specify. Some examples of stopping criteria are the total depth of the tree or the number of nodes is a tree, but the ones I typically prefer are the minimum size of a resulting leaf or specifying the minimum number of records needed to make a new split.

Lastly, I should probably mention the terminology used. When talking about decision trees, we call each of the split points a "node", with the starting node being called the **root node** and the subsequent nodes the **decision nodes**. We then refer to the final data chucks as the **leaves** of the tree, and these are with ovals in the diagram.

### Example with Code

**Terms Covered:**

Binary Splits:

:   True or False questions that split the data into two groups. Each split is determined by what split will do the best job of purifying the data.

Decision Tree:

:   A model that makes predictions by making a series of binary splits